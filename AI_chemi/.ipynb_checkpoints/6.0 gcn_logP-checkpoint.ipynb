{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#rdkit\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "from rdkit.Chem.rdMolDescriptors import CalcTPSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default option\n",
    "num_layer = 3\n",
    "hidden_dim1 = 64\n",
    "hidden_dim2 = 256\n",
    "init_lr = 0.001\n",
    "using_sc = 'gsc' # 'sc, 'gsc, 'no'\n",
    "\n",
    "if( len(sys.argv) == 6 ):\n",
    "    # Note that sys.argv[0] is gcn_logP.py\n",
    "    num_layer = int(sys.argv[1])\n",
    "    hidden_dim1 = int(sys.argv[2])\n",
    "    hidden_dim2 = int(sys.argv[3])\n",
    "    init_lr = float(sys.argv[4])\n",
    "    using_sc = sys.argv[5]              # 'sc, 'gsc, 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gcn_logP_' + str(num_layer) + '_' + str(hidden_dim1) + '_' + str(hidden_dim2) + '_' + str(init_lr) + '_' + using_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename + '.smiles', 'r')\n",
    "    contents = f.readlines()\n",
    "\n",
    "    smiles = []\n",
    "    labels = []\n",
    "    for i in contents:\n",
    "        smi = i.split()[0]\n",
    "        label = int(i.split()[2].strip())\n",
    "\n",
    "        smiles.append(smi)\n",
    "        labels.append(label)\n",
    "\n",
    "    num_total = len(smiles)\n",
    "    rand_int = np.random.randint(num_total, size=(num_total,))\n",
    "    \n",
    "    return np.asarray(smiles)[rand_int], np.asarray(labels)[rand_int]\n",
    "\n",
    "def read_ZINC(num_mol):\n",
    "    f = open('ZINC.smiles', 'r')\n",
    "    contents = f.readlines()\n",
    "\n",
    "    smi = []\n",
    "    fps = []\n",
    "    logP = []\n",
    "    tpsa = []\n",
    "    for i in range(num_mol):\n",
    "        smi = contents[i].strip()\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(m,2)\n",
    "        arr = np.zeros((1,))\n",
    "        DataStructs.ConvertToNumpyArray(fp,arr)\n",
    "        fps.append(arr)\n",
    "        logP.append(MolLogP(m))\n",
    "        tpsa.append(CalcTPSA(m))\n",
    "\n",
    "    fps = np.asarray(fps).astype(float)\n",
    "    logP = np.asarray(logP).astype(float)\n",
    "    tpsa = np.asarray(tpsa).astype(float)\n",
    "\n",
    "    return fps, logP, tpsa\n",
    "\n",
    "def read_ZINC_smiles(num_mol):\n",
    "    f = open('ZINC.smiles', 'r')\n",
    "    contents = f.readlines()\n",
    "\n",
    "    smi_list = []\n",
    "    logP_list = []\n",
    "    tpsa_list = []\n",
    "    for i in range(num_mol):\n",
    "        smi = contents[i].strip()\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        smi_list.append(smi)\n",
    "        logP_list.append(MolLogP(m))\n",
    "        tpsa_list.append(CalcTPSA(m))\n",
    "\n",
    "    logP_list = np.asarray(logP_list).astype(float)\n",
    "    tpsa_list = np.asarray(tpsa_list).astype(float)\n",
    "\n",
    "    return smi_list, logP_list, tpsa_list\n",
    "\n",
    "def smiles_to_onehot(smi_list):\n",
    "    def smiles_to_vector(smiles, vocab, max_length):\n",
    "        while len(smiles)<max_length:\n",
    "            smiles +=\" \"\n",
    "        return [vocab.index(str(x)) for x in smiles]\n",
    "\n",
    "    vocab = np.load('./vocab.npy')\n",
    "    smi_total = []\n",
    "    for smi in smi_list:\n",
    "        smi_onehot = smiles_to_vector(smi, list(vocab), 120)\n",
    "        smi_total.append(smi_onehot)\n",
    "    return np.asarray(smi_total)\n",
    "\n",
    "def convert_to_graph(smiles_list):\n",
    "    adj = []\n",
    "    adj_norm = []\n",
    "    features = []\n",
    "    maxNumAtoms = 50\n",
    "    for i in smiles_list:\n",
    "        # Mol\n",
    "        iMol = Chem.MolFromSmiles(i.strip())\n",
    "        #Adj\n",
    "        iAdjTmp = Chem.rdmolops.GetAdjacencyMatrix(iMol)\n",
    "        # Feature\n",
    "        if( iAdjTmp.shape[0] <= maxNumAtoms):\n",
    "            # Feature-preprocessing\n",
    "            iFeature = np.zeros((maxNumAtoms, 58))\n",
    "            iFeatureTmp = []\n",
    "            for atom in iMol.GetAtoms():\n",
    "                iFeatureTmp.append( atom_feature(atom) ) ### atom features only\n",
    "            iFeature[0:len(iFeatureTmp), 0:58] = iFeatureTmp ### 0 padding for feature-set\n",
    "            features.append(iFeature)\n",
    "\n",
    "            # Adj-preprocessing\n",
    "            iAdj = np.zeros((maxNumAtoms, maxNumAtoms))\n",
    "            iAdj[0:len(iFeatureTmp), 0:len(iFeatureTmp)] = iAdjTmp + np.eye(len(iFeatureTmp))\n",
    "            adj.append(np.asarray(iAdj))\n",
    "    features = np.asarray(features)\n",
    "\n",
    "    return features, adj\n",
    "    \n",
    "def atom_feature(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "                                      ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "                                       'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "                                       'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "                                       'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    [atom.GetIsAromatic()])    # (40, 6, 5, 6, 1)\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    #print list((map(lambda s: x == s, allowable_set)))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "smi_total, logP_total, tpsa_total = read_ZINC_smiles(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 30000\n",
    "num_validation = 10000\n",
    "num_test = 10000\n",
    "\n",
    "smi_train = smi_total[0:num_train]\n",
    "logP_train = logP_total[0:num_train]\n",
    "smi_validation = smi_total[num_train:(num_train+num_validation)]\n",
    "logP_validation = logP_total[num_train:(num_train+num_validation)]\n",
    "smi_test = smi_total[(num_train+num_validation):]\n",
    "logP_test = logP_total[(num_train+num_validation):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_connection(input_X, new_X, act):\n",
    "    # Skip-connection, H^(l+1)_sc = H^(l) + H^(l+1)\n",
    "    inp_dim = int(input_X.get_shape()[2])\n",
    "    out_dim = int(new_X.get_shape()[2])\n",
    "\n",
    "    if(inp_dim != out_dim):\n",
    "        output_X = act(new_X + tf.layers.dense(input_X, units=out_dim, use_bias=False))\n",
    "\n",
    "    else:\n",
    "        output_X = act(new_X + input_X)\n",
    "\n",
    "    return output_X     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gated_skip_connection(input_X, new_X, act):\n",
    "    # Skip-connection, H^(l+1)_gsc = z*H^(l) + (1-z)*H^(l+1)\n",
    "    inp_dim = int(input_X.get_shape()[2])\n",
    "    out_dim = int(new_X.get_shape()[2])\n",
    "\n",
    "    def get_gate_coefficient(input_X, new_X, out_dim):\n",
    "        X1 = tf.layers.dense(input_X, units=out_dim, use_bias=True)\n",
    "        X2 = tf.layers.dense(new_X, units=out_dim, use_bias=True)\n",
    "        gate_coefficient = tf.nn.sigmoid(X1 + X2)\n",
    "\n",
    "        return gate_coefficient\n",
    "\n",
    "    if(inp_dim != out_dim):\n",
    "        input_X = tf.layers.dense(input_X, units=out_dim, use_bias=False)\n",
    "\n",
    "    gate_coefficient = get_gate_coefficient(input_X, new_X, out_dim)\n",
    "    output_X = tf.multiply(new_X, gate_coefficient) + tf.multiply(input_X, 1.0-gate_coefficient)\n",
    "\n",
    "    return output_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_convolution(input_X, input_A, hidden_dim, act, using_sc):\n",
    "    # Graph Convolution, H^(l+1) = A{H^(l)W^(l)+b^(l))\n",
    "    output_X = tf.layers.dense(input_X,\n",
    "                               units=hidden_dim, \n",
    "                               use_bias=True,\n",
    "                               activation=None,\n",
    "                               kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    output_X = tf.matmul(input_A, output_X)\n",
    "\n",
    "    if( using_sc == 'sc' ):\n",
    "        output_X = skip_connection(input_X, output_X, act)\n",
    "\n",
    "    elif( using_sc == 'gsc' ):\n",
    "        output_X = gated_skip_connection(input_X, output_X, act)\n",
    "\n",
    "    elif( using_sc == 'no' ):\n",
    "        output_X = act(output_X)\n",
    "\n",
    "    else:\n",
    "        output_X = gated_skip_connection(input_X, output_X)\n",
    "\n",
    "    return output_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readout\n",
    "def readout(input_X, hidden_dim, act):\n",
    "    # Readout, Z = sum_{v in G} NN(H^(L)_v)\n",
    "    output_Z = tf.layers.dense(input_X, \n",
    "                               units=hidden_dim, \n",
    "                               use_bias=True,\n",
    "                               activation=None,\n",
    "                               kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    output_Z = tf.reduce_sum(output_Z, axis=1)\n",
    "    output = act(output_Z)\n",
    "\n",
    "    return output_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_atoms=50\n",
    "num_features=58\n",
    "X = tf.placeholder(tf.float64, shape=[None, num_atoms, num_features])\n",
    "A = tf.placeholder(tf.float64, shape=[None, num_atoms, num_atoms])\n",
    "Y = tf.placeholder(tf.float64, shape=[None, ])\n",
    "is_training = tf.placeholder(tf.bool, shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = X\n",
    "# Graph convolution layers\n",
    "for i in range(num_layer):\n",
    "    h = graph_convolution(h,\n",
    "                          A, \n",
    "                          hidden_dim1, \n",
    "                          tf.nn.relu,\n",
    "                          using_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readout layer\n",
    "h = readout(h, hidden_dim2, tf.nn.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor composed of MLPs(multi-layer perceptron)\n",
    "h = tf.layers.dense(h, \n",
    "                    units=hidden_dim2, \n",
    "                    use_bias=True, \n",
    "                    activation=tf.nn.relu, \n",
    "                    kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "h = tf.layers.dense(h, \n",
    "                    units=hidden_dim2, \n",
    "                    use_bias=True, \n",
    "                    activation=tf.nn.tanh, \n",
    "                    kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "Y_pred = tf.layers.dense(h, \n",
    "                         units=1, \n",
    "                         use_bias=True, \n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss\n",
    "MSE loss (L2 loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = tf.reshape(Y_pred, shape=[-1,])\n",
    "Y_pred = tf.cast(Y_pred, tf.float64)\n",
    "Y = tf.cast(Y, tf.float64)\n",
    "loss = tf.reduce_mean( (Y_pred - Y)**2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.Variable(0.0, trainable = False)  # learning rate\n",
    "opt = tf.train.AdamOptimizer(lr).minimize(loss) # Note that we use the Adam optimizer in this practice.\n",
    "#opt = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE : 0.5940971450530764 RMSE : 0.8195334772744105 Std : 0.8195319850851432 \t Training, \t Epoch : 0\n",
      "MAE : 0.34758873961662257 RMSE : 0.4500074302173122 Std : 0.4450388840893013 \t Validation, \t Epoch : 0 \t Time per epoch 25.536597967147827\n",
      "MAE : 0.2982408068793886 RMSE : 0.39276940940147753 Std : 0.3927693031130982 \t Training, \t Epoch : 1\n",
      "MAE : 0.25516955673787756 RMSE : 0.3430717893111184 Std : 0.33972559802672403 \t Validation, \t Epoch : 1 \t Time per epoch 25.148760557174683\n",
      "MAE : 0.25063025691283153 RMSE : 0.33360926167103955 Std : 0.33360881421305155 \t Training, \t Epoch : 2\n",
      "MAE : 0.21842833117296528 RMSE : 0.298449834834413 Std : 0.295648134195918 \t Validation, \t Epoch : 2 \t Time per epoch 25.338133096694946\n",
      "MAE : 0.21070845298921617 RMSE : 0.2858120676470321 Std : 0.28581195612461346 \t Training, \t Epoch : 3\n",
      "MAE : 0.19540560416551667 RMSE : 0.26997824256890157 Std : 0.26840771976705075 \t Validation, \t Epoch : 3 \t Time per epoch 25.34354066848755\n",
      "MAE : 0.19177170514789 RMSE : 0.26203818067165713 Std : 0.2620381798479046 \t Training, \t Epoch : 4\n",
      "MAE : 0.18711824953358422 RMSE : 0.26057249179268543 Std : 0.2513614645847738 \t Validation, \t Epoch : 4 \t Time per epoch 25.32795238494873\n",
      "MAE : 0.17484058348367273 RMSE : 0.2407335028278117 Std : 0.24073343793813348 \t Training, \t Epoch : 5\n",
      "MAE : 0.16402967194213688 RMSE : 0.23078154495729122 Std : 0.2288919297707054 \t Validation, \t Epoch : 5 \t Time per epoch 24.99662446975708\n",
      "MAE : 0.15560321381532868 RMSE : 0.21625959597312117 Std : 0.21625953624518787 \t Training, \t Epoch : 6\n",
      "MAE : 0.15279286004498305 RMSE : 0.21287367828870965 Std : 0.2077535271533514 \t Validation, \t Epoch : 6 \t Time per epoch 24.981173515319824\n",
      "MAE : 0.14378921505545367 RMSE : 0.20035232653281485 Std : 0.20035232067099332 \t Training, \t Epoch : 7\n",
      "MAE : 0.14369971062266315 RMSE : 0.20046891310012965 Std : 0.19692945136659487 \t Validation, \t Epoch : 7 \t Time per epoch 24.957663536071777\n",
      "MAE : 0.13617902187397682 RMSE : 0.18969259251277445 Std : 0.1896925908966906 \t Training, \t Epoch : 8\n",
      "MAE : 0.13083987186438678 RMSE : 0.18526783527264784 Std : 0.18484290840198211 \t Validation, \t Epoch : 8 \t Time per epoch 25.009562015533447\n",
      "MAE : 0.1297369038712589 RMSE : 0.18028554844097952 Std : 0.1802855470134195 \t Training, \t Epoch : 9\n",
      "MAE : 0.13034836537734448 RMSE : 0.18343574215547157 Std : 0.17910480088756875 \t Validation, \t Epoch : 9 \t Time per epoch 24.972506761550903\n",
      "MAE : 0.12112927367384138 RMSE : 0.16938254714640663 Std : 0.1693825461654286 \t Training, \t Epoch : 10\n",
      "MAE : 0.11865957212949829 RMSE : 0.17086861759247193 Std : 0.17077419054997708 \t Validation, \t Epoch : 10 \t Time per epoch 25.02671480178833\n",
      "MAE : 0.11491068479754213 RMSE : 0.16123835811450965 Std : 0.1612383545504904 \t Training, \t Epoch : 11\n",
      "MAE : 0.11343170162687631 RMSE : 0.1658537301441548 Std : 0.16585046016379212 \t Validation, \t Epoch : 11 \t Time per epoch 25.301820755004883\n",
      "MAE : 0.11018119618423554 RMSE : 0.1546178531274209 Std : 0.1546178436962325 \t Training, \t Epoch : 12\n",
      "MAE : 0.11037962597619862 RMSE : 0.1614702189761279 Std : 0.16146695005839112 \t Validation, \t Epoch : 12 \t Time per epoch 25.708962202072144\n",
      "MAE : 0.10634845084262619 RMSE : 0.1489235387618813 Std : 0.1489235351738891 \t Training, \t Epoch : 13\n",
      "MAE : 0.10848000387023296 RMSE : 0.15877265408880578 Std : 0.15765689115706022 \t Validation, \t Epoch : 13 \t Time per epoch 25.813157558441162\n",
      "MAE : 0.11100722437315633 RMSE : 0.15279718884562243 Std : 0.15279718792086083 \t Training, \t Epoch : 14\n",
      "MAE : 0.11834752409439543 RMSE : 0.16749100274286136 Std : 0.1597219573936497 \t Validation, \t Epoch : 14 \t Time per epoch 25.62662172317505\n",
      "MAE : 0.10888205420035928 RMSE : 0.1487681729949104 Std : 0.14876815069250426 \t Training, \t Epoch : 15\n",
      "MAE : 0.11522195904697119 RMSE : 0.1644451974232013 Std : 0.15798240621835458 \t Validation, \t Epoch : 15 \t Time per epoch 25.657810926437378\n",
      "MAE : 0.10313580811778457 RMSE : 0.14052466476066353 Std : 0.14052466063859803 \t Training, \t Epoch : 16\n",
      "MAE : 0.11035676225877321 RMSE : 0.16017150136872607 Std : 0.15903358134319062 \t Validation, \t Epoch : 16 \t Time per epoch 25.87876868247986\n",
      "MAE : 0.09636278712063259 RMSE : 0.13158279838351317 Std : 0.13158279193784622 \t Training, \t Epoch : 17\n",
      "MAE : 0.1080459079995849 RMSE : 0.15794393512046434 Std : 0.15784936194800378 \t Validation, \t Epoch : 17 \t Time per epoch 25.83099627494812\n",
      "MAE : 0.08797263499715262 RMSE : 0.12179265161505795 Std : 0.12179265121215477 \t Training, \t Epoch : 18\n",
      "MAE : 0.10522613105158989 RMSE : 0.15495043807848596 Std : 0.15173430196099802 \t Validation, \t Epoch : 18 \t Time per epoch 25.6740562915802\n",
      "MAE : 0.0829095716853255 RMSE : 0.11533510809846208 Std : 0.1153350786335182 \t Training, \t Epoch : 19\n",
      "MAE : 0.10202904388063787 RMSE : 0.1509960310503023 Std : 0.14711363357196622 \t Validation, \t Epoch : 19 \t Time per epoch 25.613365411758423\n",
      "MAE : 0.08022759759709014 RMSE : 0.11134490882544683 Std : 0.11134490190908047 \t Training, \t Epoch : 20\n",
      "MAE : 0.10061981431994035 RMSE : 0.1491436211072981 Std : 0.14488255748523227 \t Validation, \t Epoch : 20 \t Time per epoch 25.625806093215942\n",
      "MAE : 0.07770613716445218 RMSE : 0.10740888316256898 Std : 0.10740888251811119 \t Training, \t Epoch : 21\n",
      "MAE : 0.09977885617298898 RMSE : 0.14663437699909734 Std : 0.14025060257972466 \t Validation, \t Epoch : 21 \t Time per epoch 25.608970165252686\n",
      "MAE : 0.07603507804873107 RMSE : 0.10439522633256655 Std : 0.10439522578663689 \t Training, \t Epoch : 22\n",
      "MAE : 0.09679625431185726 RMSE : 0.14300550990409852 Std : 0.13898329177248592 \t Validation, \t Epoch : 22 \t Time per epoch 25.523804426193237\n",
      "MAE : 0.07439256180882102 RMSE : 0.10164158053875705 Std : 0.1016415795910258 \t Training, \t Epoch : 23\n",
      "MAE : 0.09469899836320211 RMSE : 0.1408212791772692 Std : 0.1390408767438505 \t Validation, \t Epoch : 23 \t Time per epoch 25.711056232452393\n",
      "MAE : 0.07449740886525458 RMSE : 0.10131244270528403 Std : 0.10131243900204528 \t Training, \t Epoch : 24\n",
      "MAE : 0.09490624551002659 RMSE : 0.1407995494124224 Std : 0.14065440623980383 \t Validation, \t Epoch : 24 \t Time per epoch 25.625810861587524\n",
      "MAE : 0.07392993148585879 RMSE : 0.10026141862200606 Std : 0.10026136715497634 \t Training, \t Epoch : 25\n",
      "MAE : 0.10143573960521826 RMSE : 0.14524050139315234 Std : 0.14111081000549705 \t Validation, \t Epoch : 25 \t Time per epoch 25.453742027282715\n",
      "MAE : 0.07244734061562824 RMSE : 0.0981864009600051 Std : 0.09818635019257514 \t Training, \t Epoch : 26\n",
      "MAE : 0.09589424447446596 RMSE : 0.1402645596176244 Std : 0.1372655235881631 \t Validation, \t Epoch : 26 \t Time per epoch 25.588277578353882\n",
      "MAE : 0.06763692894134238 RMSE : 0.09217668327038592 Std : 0.09217668157137271 \t Training, \t Epoch : 27\n",
      "MAE : 0.09155678353784917 RMSE : 0.13698309170681552 Std : 0.13605694863462678 \t Validation, \t Epoch : 27 \t Time per epoch 25.5063579082489\n",
      "MAE : 0.06302690384511801 RMSE : 0.08633503274351814 Std : 0.08633503270339354 \t Training, \t Epoch : 28\n",
      "MAE : 0.08711673300338238 RMSE : 0.1327687052714205 Std : 0.1326101474491524 \t Validation, \t Epoch : 28 \t Time per epoch 25.554792881011963\n",
      "MAE : 0.060578451120778615 RMSE : 0.08295230570662526 Std : 0.08295230541803869 \t Training, \t Epoch : 29\n",
      "MAE : 0.08617531196852296 RMSE : 0.13234271868003716 Std : 0.13224378540592274 \t Validation, \t Epoch : 29 \t Time per epoch 25.548890829086304\n",
      "MAE : 0.05777751593798438 RMSE : 0.07942025908886913 Std : 0.07942025895963563 \t Training, \t Epoch : 30\n",
      "MAE : 0.08412723037959423 RMSE : 0.13080256925916628 Std : 0.13080237275482226 \t Validation, \t Epoch : 30 \t Time per epoch 25.523266553878784\n",
      "MAE : 0.056613946721290914 RMSE : 0.07782710686065578 Std : 0.07782710493412841 \t Training, \t Epoch : 31\n",
      "MAE : 0.08356132274480545 RMSE : 0.1298842508962203 Std : 0.1298383955198798 \t Validation, \t Epoch : 31 \t Time per epoch 25.539318799972534\n",
      "MAE : 0.055058813674106304 RMSE : 0.07574360067446975 Std : 0.07574359994182693 \t Training, \t Epoch : 32\n",
      "MAE : 0.08187245627742339 RMSE : 0.128345053542226 Std : 0.12821257324885485 \t Validation, \t Epoch : 32 \t Time per epoch 25.537330150604248\n",
      "MAE : 0.05416158111680056 RMSE : 0.07459161671390308 Std : 0.07459161627541447 \t Training, \t Epoch : 33\n",
      "MAE : 0.08126886930583437 RMSE : 0.1274973449910713 Std : 0.1273392160631138 \t Validation, \t Epoch : 33 \t Time per epoch 25.578940868377686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE : 0.05332254347704845 RMSE : 0.07318689093245272 Std : 0.07318689013053194 \t Training, \t Epoch : 34\n",
      "MAE : 0.07944439345231155 RMSE : 0.12616855340374056 Std : 0.12616757295424733 \t Validation, \t Epoch : 34 \t Time per epoch 25.540971517562866\n",
      "MAE : 0.05489333134876553 RMSE : 0.07452002598208517 Std : 0.07452002243842665 \t Training, \t Epoch : 35\n",
      "MAE : 0.07976335969727472 RMSE : 0.12604313249329113 Std : 0.12587628073105406 \t Validation, \t Epoch : 35 \t Time per epoch 25.71082043647766\n",
      "MAE : 0.059692239475061665 RMSE : 0.07988916033340351 Std : 0.07988915927343661 \t Training, \t Epoch : 36\n",
      "MAE : 0.08602137468128972 RMSE : 0.13245552929282098 Std : 0.1265952633050764 \t Validation, \t Epoch : 36 \t Time per epoch 25.581843852996826\n",
      "MAE : 0.05841465312189463 RMSE : 0.0786031674297617 Std : 0.07860311543029902 \t Training, \t Epoch : 37\n",
      "MAE : 0.09317048535355005 RMSE : 0.13739108442961886 Std : 0.12594517379222622 \t Validation, \t Epoch : 37 \t Time per epoch 25.51815390586853\n",
      "MAE : 0.05383078469088722 RMSE : 0.07314243895257662 Std : 0.07314242924761201 \t Training, \t Epoch : 38\n",
      "MAE : 0.09206739573612474 RMSE : 0.13653747283647347 Std : 0.1258230614610258 \t Validation, \t Epoch : 38 \t Time per epoch 25.529030323028564\n",
      "MAE : 0.05063017185803929 RMSE : 0.069298404158566 Std : 0.06929840322931843 \t Training, \t Epoch : 39\n",
      "MAE : 0.09083793168739623 RMSE : 0.13538454955519968 Std : 0.12567852117723727 \t Validation, \t Epoch : 39 \t Time per epoch 25.647912979125977\n",
      "MAE : 0.04810951021922374 RMSE : 0.06613821606501585 Std : 0.06613821412854583 \t Training, \t Epoch : 40\n",
      "MAE : 0.09002782337666596 RMSE : 0.1343319700520742 Std : 0.12501033076956902 \t Validation, \t Epoch : 40 \t Time per epoch 25.550071239471436\n",
      "MAE : 0.046748750409785246 RMSE : 0.06426855070712069 Std : 0.06426853804565796 \t Training, \t Epoch : 41\n",
      "MAE : 0.09051789568715278 RMSE : 0.1349689465362293 Std : 0.1248984922568185 \t Validation, \t Epoch : 41 \t Time per epoch 25.62508797645569\n",
      "MAE : 0.04539728088376834 RMSE : 0.06242558961476339 Std : 0.062425585707631036 \t Training, \t Epoch : 42\n",
      "MAE : 0.08809747369744404 RMSE : 0.13217780103127572 Std : 0.12346154461033466 \t Validation, \t Epoch : 42 \t Time per epoch 25.753288745880127\n",
      "MAE : 0.04417566791206449 RMSE : 0.060767720858544066 Std : 0.06076771631822645 \t Training, \t Epoch : 43\n",
      "MAE : 0.08705705392491929 RMSE : 0.13230728934233144 Std : 0.12418303753975098 \t Validation, \t Epoch : 43 \t Time per epoch 25.464898347854614\n",
      "MAE : 0.04320960386426937 RMSE : 0.059377717451585 Std : 0.05937771308688679 \t Training, \t Epoch : 44\n",
      "MAE : 0.08594349644185363 RMSE : 0.13057494878534384 Std : 0.12294053784662815 \t Validation, \t Epoch : 44 \t Time per epoch 25.37196183204651\n",
      "MAE : 0.04235983202353056 RMSE : 0.058157723642924375 Std : 0.058157721045417096 \t Training, \t Epoch : 45\n",
      "MAE : 0.08522009692000079 RMSE : 0.13047320591562234 Std : 0.12342244578977916 \t Validation, \t Epoch : 45 \t Time per epoch 25.427333116531372\n",
      "MAE : 0.04153113900585878 RMSE : 0.05692566539247791 Std : 0.05692565815849124 \t Training, \t Epoch : 46\n",
      "MAE : 0.08405744994539746 RMSE : 0.12912058977301022 Std : 0.12237184335763461 \t Validation, \t Epoch : 46 \t Time per epoch 25.38921880722046\n",
      "MAE : 0.040895537439391165 RMSE : 0.05593875100717852 Std : 0.05593875044175107 \t Training, \t Epoch : 47\n",
      "MAE : 0.08349629418170197 RMSE : 0.12864855496381206 Std : 0.12225928949723613 \t Validation, \t Epoch : 47 \t Time per epoch 25.416351556777954\n",
      "MAE : 0.04025799049964661 RMSE : 0.05503090705566386 Std : 0.05503089720549724 \t Training, \t Epoch : 48\n",
      "MAE : 0.0816373171758833 RMSE : 0.1273970473997117 Std : 0.12193497875619651 \t Validation, \t Epoch : 48 \t Time per epoch 25.273974657058716\n",
      "MAE : 0.03973936309428086 RMSE : 0.05423852049880968 Std : 0.05423852002509195 \t Training, \t Epoch : 49\n",
      "MAE : 0.080615522703161 RMSE : 0.1267487875294082 Std : 0.12171368566897033 \t Validation, \t Epoch : 49 \t Time per epoch 25.421306610107422\n",
      "MAE : 0.039229148922656075 RMSE : 0.05354648429740574 Std : 0.05354647729074417 \t Training, \t Epoch : 50\n",
      "MAE : 0.07799577869981952 RMSE : 0.12476812390262053 Std : 0.12110588429909377 \t Validation, \t Epoch : 50 \t Time per epoch 25.332926273345947\n",
      "MAE : 0.03869749109920285 RMSE : 0.05282677734255726 Std : 0.05282677734180514 \t Training, \t Epoch : 51\n",
      "MAE : 0.07487879668345172 RMSE : 0.122495435321042 Std : 0.12067275607446412 \t Validation, \t Epoch : 51 \t Time per epoch 25.389029502868652\n",
      "MAE : 0.03845435389546764 RMSE : 0.05256575754674829 Std : 0.05256575616061215 \t Training, \t Epoch : 52\n",
      "MAE : 0.07217736920037314 RMSE : 0.12032535302920834 Std : 0.1201210120447333 \t Validation, \t Epoch : 52 \t Time per epoch 25.41322612762451\n",
      "MAE : 0.0379540801917954 RMSE : 0.05192115172629868 Std : 0.05192113607562352 \t Training, \t Epoch : 53\n",
      "MAE : 0.07186030237921189 RMSE : 0.12012032067090953 Std : 0.12001504938717644 \t Validation, \t Epoch : 53 \t Time per epoch 25.508774280548096\n",
      "MAE : 0.03718626023077472 RMSE : 0.05102897916394332 Std : 0.05102897006344358 \t Training, \t Epoch : 54\n",
      "MAE : 0.07235787835796134 RMSE : 0.12057203394865325 Std : 0.12012597754015147 \t Validation, \t Epoch : 54 \t Time per epoch 25.567591667175293\n",
      "MAE : 0.036046895908058776 RMSE : 0.04967139557513067 Std : 0.0496713932565225 \t Training, \t Epoch : 55\n",
      "MAE : 0.07216241714913968 RMSE : 0.1201885906761369 Std : 0.11975948720464037 \t Validation, \t Epoch : 55 \t Time per epoch 25.547093152999878\n",
      "MAE : 0.035174293263514 RMSE : 0.04861216114531239 Std : 0.04861216110722661 \t Training, \t Epoch : 56\n",
      "MAE : 0.0720787896666281 RMSE : 0.12037435066509684 Std : 0.11999719607643616 \t Validation, \t Epoch : 56 \t Time per epoch 25.527559995651245\n",
      "MAE : 0.034364859418233425 RMSE : 0.04757537402753086 Std : 0.04757537360640652 \t Training, \t Epoch : 57\n",
      "MAE : 0.07148624476359329 RMSE : 0.11980544920579324 Std : 0.11959141984236506 \t Validation, \t Epoch : 57 \t Time per epoch 25.479459524154663\n",
      "MAE : 0.03387489821223101 RMSE : 0.04691879845618971 Std : 0.04691879825375289 \t Training, \t Epoch : 58\n",
      "MAE : 0.07139005511714008 RMSE : 0.11988939238103187 Std : 0.11973876856250311 \t Validation, \t Epoch : 58 \t Time per epoch 25.507176399230957\n",
      "MAE : 0.03327548483195191 RMSE : 0.04616092843084881 Std : 0.0461609276743756 \t Training, \t Epoch : 59\n",
      "MAE : 0.07093552463304467 RMSE : 0.11957496294430481 Std : 0.11950926555147437 \t Validation, \t Epoch : 59 \t Time per epoch 25.481722831726074\n",
      "MAE : 0.03292155492032299 RMSE : 0.045652728424290336 Std : 0.04565272834014132 \t Training, \t Epoch : 60\n",
      "MAE : 0.07077989325408132 RMSE : 0.11938092169719308 Std : 0.11935243532208344 \t Validation, \t Epoch : 60 \t Time per epoch 25.693322896957397\n",
      "MAE : 0.03246018586094259 RMSE : 0.04505334513765443 Std : 0.045053344726804466 \t Training, \t Epoch : 61\n",
      "MAE : 0.07043890177179783 RMSE : 0.11916102203435912 Std : 0.11916101856315053 \t Validation, \t Epoch : 61 \t Time per epoch 25.535313844680786\n",
      "MAE : 0.03223780988406287 RMSE : 0.04469547701516267 Std : 0.04469547696902332 \t Training, \t Epoch : 62\n",
      "MAE : 0.07038690329171819 RMSE : 0.11912433671744904 Std : 0.11910398265377249 \t Validation, \t Epoch : 62 \t Time per epoch 25.49853277206421\n",
      "MAE : 0.03194314202891918 RMSE : 0.04426328509648686 Std : 0.04426328386780761 \t Training, \t Epoch : 63\n",
      "MAE : 0.07041763043092673 RMSE : 0.11919915474089285 Std : 0.11894628504220585 \t Validation, \t Epoch : 63 \t Time per epoch 25.679872035980225\n",
      "MAE : 0.03178683684210033 RMSE : 0.04399998731780947 Std : 0.04399998136677948 \t Training, \t Epoch : 64\n",
      "MAE : 0.07110181942577433 RMSE : 0.11965103692223109 Std : 0.1187301706547668 \t Validation, \t Epoch : 64 \t Time per epoch 25.52185893058777\n",
      "MAE : 0.031684582625177656 RMSE : 0.04379812197278299 Std : 0.04379809541833949 \t Training, \t Epoch : 65\n",
      "MAE : 0.07265869716276149 RMSE : 0.12101580140546371 Std : 0.11885695374709086 \t Validation, \t Epoch : 65 \t Time per epoch 25.573757648468018\n",
      "MAE : 0.03162513157192647 RMSE : 0.04365973166556903 Std : 0.04365969437467163 \t Training, \t Epoch : 66\n",
      "MAE : 0.07321205772051144 RMSE : 0.1211613340856238 Std : 0.11851517309239205 \t Validation, \t Epoch : 66 \t Time per epoch 25.598950147628784\n",
      "MAE : 0.031241946879341426 RMSE : 0.04321915916128698 Std : 0.043219142848486464 \t Training, \t Epoch : 67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE : 0.07231131644222749 RMSE : 0.12052365593038598 Std : 0.1186163994484942 \t Validation, \t Epoch : 67 \t Time per epoch 25.572788953781128\n",
      "MAE : 0.03067610496821805 RMSE : 0.042564532945375114 Std : 0.042564529705262096 \t Training, \t Epoch : 68\n",
      "MAE : 0.07096081898232708 RMSE : 0.11937278682533703 Std : 0.11858193195564744 \t Validation, \t Epoch : 68 \t Time per epoch 25.585286855697632\n",
      "MAE : 0.030040965571378732 RMSE : 0.04184818641209349 Std : 0.04184818554455248 \t Training, \t Epoch : 69\n",
      "MAE : 0.0703626310547072 RMSE : 0.1188895663623733 Std : 0.1185317339619632 \t Validation, \t Epoch : 69 \t Time per epoch 25.583359241485596\n",
      "MAE : 0.02962493732176697 RMSE : 0.04133049316741398 Std : 0.041330493051833055 \t Training, \t Epoch : 70\n",
      "MAE : 0.07015600230928083 RMSE : 0.11879113796334916 Std : 0.11862112162022163 \t Validation, \t Epoch : 70 \t Time per epoch 25.702427864074707\n",
      "MAE : 0.029222972057719558 RMSE : 0.04083531469691877 Std : 0.04083531334951404 \t Training, \t Epoch : 71\n",
      "MAE : 0.06998836803373333 RMSE : 0.11857989788331252 Std : 0.11848924899181804 \t Validation, \t Epoch : 71 \t Time per epoch 25.580515384674072\n",
      "MAE : 0.0288923531287021 RMSE : 0.04043903242544984 Std : 0.04043903197937911 \t Training, \t Epoch : 72\n",
      "MAE : 0.06987619067231604 RMSE : 0.11862488635793218 Std : 0.11856113686233723 \t Validation, \t Epoch : 72 \t Time per epoch 25.492210388183594\n",
      "MAE : 0.028629219714849403 RMSE : 0.04011104894751618 Std : 0.04011104894693522 \t Training, \t Epoch : 73\n",
      "MAE : 0.06974997879778154 RMSE : 0.11848645669469322 Std : 0.11845538458087664 \t Validation, \t Epoch : 73 \t Time per epoch 25.64610004425049\n",
      "MAE : 0.02834038521873862 RMSE : 0.039764741074767476 Std : 0.03976474096095506 \t Training, \t Epoch : 74\n",
      "MAE : 0.06958229460771474 RMSE : 0.11841570140098617 Std : 0.11839829028131713 \t Validation, \t Epoch : 74 \t Time per epoch 25.574798583984375\n",
      "MAE : 0.02812104148138677 RMSE : 0.03948327953645508 Std : 0.03948327933256419 \t Training, \t Epoch : 75\n",
      "MAE : 0.06947009939457988 RMSE : 0.11840576274356134 Std : 0.11840163906183639 \t Validation, \t Epoch : 75 \t Time per epoch 25.598525524139404\n",
      "MAE : 0.027907255340962438 RMSE : 0.039224642013644485 Std : 0.03922464197939444 \t Training, \t Epoch : 76\n",
      "MAE : 0.0692701900551955 RMSE : 0.11821198471995002 Std : 0.11821179073939976 \t Validation, \t Epoch : 76 \t Time per epoch 25.57148241996765\n",
      "MAE : 0.027707378533083648 RMSE : 0.038965535176755474 Std : 0.038965534900477945 \t Training, \t Epoch : 77\n",
      "MAE : 0.06911205709754217 RMSE : 0.11814840511464822 Std : 0.11814574086753671 \t Validation, \t Epoch : 77 \t Time per epoch 25.59904956817627\n",
      "MAE : 0.02755732022983713 RMSE : 0.03876275033712656 Std : 0.038762750204904495 \t Training, \t Epoch : 78\n",
      "MAE : 0.06892277451222671 RMSE : 0.11792352873264483 Std : 0.11791425588212914 \t Validation, \t Epoch : 78 \t Time per epoch 25.482218027114868\n",
      "MAE : 0.027422780016183625 RMSE : 0.03857934886450271 Std : 0.03857934883517769 \t Training, \t Epoch : 79\n",
      "MAE : 0.06877158009748201 RMSE : 0.11784219683839353 Std : 0.11782093158914163 \t Validation, \t Epoch : 79 \t Time per epoch 25.556580781936646\n",
      "MAE : 0.0273894765681651 RMSE : 0.03849016453893797 Std : 0.03849016423245365 \t Training, \t Epoch : 80\n",
      "MAE : 0.06859359812660883 RMSE : 0.11766108346360915 Std : 0.11763352971576399 \t Validation, \t Epoch : 80 \t Time per epoch 25.359527349472046\n",
      "MAE : 0.027440533468838325 RMSE : 0.03849235801072184 Std : 0.03849235764453404 \t Training, \t Epoch : 81\n",
      "MAE : 0.06837203046531501 RMSE : 0.11752257833946392 Std : 0.1175006654219077 \t Validation, \t Epoch : 81 \t Time per epoch 25.373576164245605\n",
      "MAE : 0.027525767917904155 RMSE : 0.038545172730629094 Std : 0.038545171685977916 \t Training, \t Epoch : 82\n",
      "MAE : 0.06806760655872983 RMSE : 0.11734087168282234 Std : 0.11733863501480118 \t Validation, \t Epoch : 82 \t Time per epoch 25.463497638702393\n",
      "MAE : 0.027455913132439672 RMSE : 0.038432107936729396 Std : 0.038432107479419345 \t Training, \t Epoch : 83\n",
      "MAE : 0.06778559091271659 RMSE : 0.11712658466321631 Std : 0.11709863960691683 \t Validation, \t Epoch : 83 \t Time per epoch 25.36419153213501\n",
      "MAE : 0.02722074626278261 RMSE : 0.038152310444998346 Std : 0.03815230821320223 \t Training, \t Epoch : 84\n",
      "MAE : 0.06766991163911884 RMSE : 0.11707242050166582 Std : 0.11697556252797114 \t Validation, \t Epoch : 84 \t Time per epoch 25.294690370559692\n",
      "MAE : 0.02695270233738029 RMSE : 0.03783909292695436 Std : 0.03783909105182733 \t Training, \t Epoch : 85\n",
      "MAE : 0.06756975025845882 RMSE : 0.11691478245880531 Std : 0.11674135309331829 \t Validation, \t Epoch : 85 \t Time per epoch 25.397224187850952\n",
      "MAE : 0.02670991221261535 RMSE : 0.03755536363707039 Std : 0.03755536149935219 \t Training, \t Epoch : 86\n",
      "MAE : 0.06749807941048003 RMSE : 0.11684081336076797 Std : 0.11660126405866914 \t Validation, \t Epoch : 86 \t Time per epoch 25.416540384292603\n",
      "MAE : 0.02650002223661127 RMSE : 0.037307900592293955 Std : 0.037307898184769636 \t Training, \t Epoch : 87\n",
      "MAE : 0.06739115479940416 RMSE : 0.11664195766214633 Std : 0.1163633247385377 \t Validation, \t Epoch : 87 \t Time per epoch 25.358428955078125\n",
      "MAE : 0.02630520325721927 RMSE : 0.037079865213131266 Std : 0.037079863247234626 \t Training, \t Epoch : 88\n",
      "MAE : 0.06729155475084203 RMSE : 0.11649383736260885 Std : 0.11619318482332912 \t Validation, \t Epoch : 88 \t Time per epoch 25.37675166130066\n",
      "MAE : 0.026138909178067275 RMSE : 0.036874630325884894 Std : 0.03687462867385205 \t Training, \t Epoch : 89\n",
      "MAE : 0.06712552283933873 RMSE : 0.11627814390989447 Std : 0.115986505898524 \t Validation, \t Epoch : 89 \t Time per epoch 25.27621817588806\n",
      "MAE : 0.025996528831972305 RMSE : 0.036693342301691814 Std : 0.03669334094441233 \t Training, \t Epoch : 90\n",
      "MAE : 0.06698325305046886 RMSE : 0.11609731235924697 Std : 0.11582831055243327 \t Validation, \t Epoch : 90 \t Time per epoch 25.272547006607056\n",
      "MAE : 0.02587030812809694 RMSE : 0.036526681302183 Std : 0.036526679645360015 \t Training, \t Epoch : 91\n",
      "MAE : 0.06679473198836422 RMSE : 0.1158798131149475 Std : 0.11563971682096118 \t Validation, \t Epoch : 91 \t Time per epoch 25.236289024353027\n",
      "MAE : 0.02575811844488016 RMSE : 0.03637642179857787 Std : 0.03637642052825204 \t Training, \t Epoch : 92\n",
      "MAE : 0.06662978064567696 RMSE : 0.11568978460153272 Std : 0.11548518011273104 \t Validation, \t Epoch : 92 \t Time per epoch 25.240712881088257\n",
      "MAE : 0.02565722179115907 RMSE : 0.03623672263899636 Std : 0.036236722294218704 \t Training, \t Epoch : 93\n",
      "MAE : 0.06645009200345571 RMSE : 0.11546639654712496 Std : 0.11530128892804495 \t Validation, \t Epoch : 93 \t Time per epoch 25.358940362930298\n",
      "MAE : 0.025578963772352404 RMSE : 0.03612274295473968 Std : 0.03612274220518751 \t Training, \t Epoch : 94\n",
      "MAE : 0.06627950915676196 RMSE : 0.11527180877620433 Std : 0.11514150748559455 \t Validation, \t Epoch : 94 \t Time per epoch 25.258170127868652\n",
      "MAE : 0.025495911323410586 RMSE : 0.03599463662133898 Std : 0.03599463649105557 \t Training, \t Epoch : 95\n",
      "MAE : 0.06615086267751044 RMSE : 0.1151336130323237 Std : 0.11503124887060469 \t Validation, \t Epoch : 95 \t Time per epoch 25.52532124519348\n",
      "MAE : 0.02543521775184538 RMSE : 0.035891693957227556 Std : 0.035891693668450256 \t Training, \t Epoch : 96\n",
      "MAE : 0.06601238830003314 RMSE : 0.11498127013414834 Std : 0.11490160155230071 \t Validation, \t Epoch : 96 \t Time per epoch 25.518778562545776\n",
      "MAE : 0.025353943567913047 RMSE : 0.03576723043459836 Std : 0.03576722982802951 \t Training, \t Epoch : 97\n",
      "MAE : 0.0658742737298628 RMSE : 0.1148125051724123 Std : 0.11474826706260696 \t Validation, \t Epoch : 97 \t Time per epoch 25.481351613998413\n",
      "MAE : 0.025274062646887776 RMSE : 0.035649186729552765 Std : 0.035649186251142946 \t Training, \t Epoch : 98\n",
      "MAE : 0.06577396393278559 RMSE : 0.11472317992110186 Std : 0.11466814871522393 \t Validation, \t Epoch : 98 \t Time per epoch 25.589383602142334\n",
      "MAE : 0.02518447556203865 RMSE : 0.035525623273924535 Std : 0.03552562277119485 \t Training, \t Epoch : 99\n",
      "MAE : 0.0656509748391171 RMSE : 0.11460142873216875 Std : 0.11455744809610698 \t Validation, \t Epoch : 99 \t Time per epoch 25.413315296173096\n"
     ]
    }
   ],
   "source": [
    "#5. Training & validation\n",
    "batch_size = 100\n",
    "epoch_size = 100\n",
    "decay_rate = 0.95\n",
    "batch_train = int(num_train/batch_size)\n",
    "batch_validation = int(num_validation/batch_size)\n",
    "batch_test = int(num_test/batch_size)\n",
    "\n",
    "total_iter = 0\n",
    "total_time = 0.0\n",
    "for t in range(epoch_size):\n",
    "\n",
    "    pred_train = []\n",
    "    sess.run(tf.assign( lr, init_lr*( decay_rate**t ) ))\n",
    "    st = time.time()\n",
    "    for i in range(batch_train):\n",
    "        total_iter += 1\n",
    "        smi_batch = smi_train[i*batch_size:(i+1)*batch_size]\n",
    "        X_batch, A_batch = convert_to_graph(smi_batch)\n",
    "        Y_batch = logP_train[i*batch_size:(i+1)*batch_size]\n",
    "        _opt, _Y, _loss = sess.run([opt, Y_pred, loss], feed_dict = {X : X_batch, A : A_batch, Y : Y_batch, is_training : True})\n",
    "        pred_train.append(_Y.flatten())\n",
    "        #print(\"Epoch :\", t, \"\\t batch:\", i, \"Loss :\", _loss, \"\\t Training\")\n",
    "    pred_train = np.concatenate(pred_train, axis=0)\n",
    "    error = (logP_train-pred_train)\n",
    "    mae = np.mean(np.abs(error))\n",
    "    rmse = np.sqrt(np.mean(error**2))\n",
    "    stdv = np.std(error)\n",
    "    print (\"MAE :\", mae, \"RMSE :\", rmse, \"Std :\", stdv, \"\\t Training, \\t Epoch :\", t)\n",
    "\n",
    "    pred_validation = []\n",
    "    for i in range(batch_validation):\n",
    "        smi_batch = smi_validation[i*batch_size:(i+1)*batch_size]\n",
    "        X_batch, A_batch = convert_to_graph(smi_batch)\n",
    "        Y_batch = logP_validation[i*batch_size:(i+1)*batch_size]\n",
    "        _Y, _loss = sess.run([Y_pred, loss], feed_dict = {X : X_batch, A : A_batch, Y : Y_batch, is_training : False})\n",
    "        #print(\"Epoch :\", t, \"\\t batch:\", i, \"Loss :\", _loss, \"\\t validation\")\n",
    "        pred_validation.append(_Y.flatten())\n",
    "\n",
    "    pred_validation = np.concatenate(pred_validation, axis=0)\n",
    "    error = (logP_validation-pred_validation)\n",
    "    mae = np.mean(np.abs(error))\n",
    "    rmse = np.sqrt(np.mean(error**2))\n",
    "    stdv = np.std(error)\n",
    "\n",
    "    et = time.time()\n",
    "    print (\"MAE :\", mae, \"RMSE :\", rmse, \"Std :\", stdv, \"\\t Validation, \\t Epoch :\", t, \"\\t Time per epoch\", (et-st))\n",
    "    total_time += (et-st)\n",
    "\n",
    "    ### save model\n",
    "    ckpt_path = 'save/'+model_name+'.ckpt'\n",
    "    saver.save(sess, ckpt_path, global_step=total_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 0.06647435837798822 RMSE : 0.11542036090860641 Std : 0.11537884549722639 \t Test \t Total time : 2547.8703486919403\n"
     ]
    }
   ],
   "source": [
    "#6. Test\n",
    "pred_test = []\n",
    "for i in range(batch_test):\n",
    "    smi_batch = smi_test[i*batch_size:(i+1)*batch_size]\n",
    "    X_batch, A_batch = convert_to_graph(smi_batch)\n",
    "    Y_batch = logP_test[i*batch_size:(i+1)*batch_size]\n",
    "    _Y, _loss = sess.run([Y_pred, loss], feed_dict = {X : X_batch, A : A_batch, Y : Y_batch, is_training : False})\n",
    "    pred_test.append(_Y.flatten())\n",
    "\n",
    "pred_test = np.concatenate(pred_test, axis=0)\n",
    "error = (logP_test-pred_test)\n",
    "mae = np.mean(np.abs(error))\n",
    "rmse = np.sqrt(np.mean(error**2))\n",
    "stdv = np.std(error)\n",
    "\n",
    "print (\"MSE :\", mae, \"RMSE :\", rmse, \"Std :\", stdv, \"\\t Test\", \"\\t Total time :\", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(logP_test, pred_test, s=3)\n",
    "plt.xlabel('logP - Truth', fontsize=15)\n",
    "plt.ylabel('logP - Prediction', fontsize=15)\n",
    "x = np.arange(-4,6)\n",
    "plt.plot(x,x,c='black')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./'+model_name+'_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
