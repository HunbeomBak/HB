{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "## Standard LeNet5 with TensorFlow\n",
    "### Xavier Bresson, Sept. 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the original LeNet5 Convolutional Neural Networks:<br>\n",
    "Gradient-based learning applied to document recognition<br>\n",
    "Y LeCun, L Bottou, Y Bengio, P Haffner<br>\n",
    "Proceedings of the IEEE 86 (11), 2278-2324<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000,)\n",
      "(5000, 784)\n",
      "(5000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('datasets', one_hot=False) # load data in folder datasets/\n",
    "\n",
    "train_data = mnist.train.images.astype(np.float32)\n",
    "val_data = mnist.validation.images.astype(np.float32)\n",
    "test_data = mnist.test.images.astype(np.float32)\n",
    "train_labels = mnist.train.labels\n",
    "val_labels = mnist.validation.labels\n",
    "test_labels = mnist.test.labels\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet LeNet5\n",
    "### CL32-MP4-CL64-MP4-FC512-FC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ConvNet_LeNet5(object):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, net_parameters):\n",
    "        \n",
    "        print('ConvNet: LeNet5')\n",
    "        \n",
    "        # parameters\n",
    "        Nx, Ny, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F = net_parameters\n",
    "        self.Nx = Nx\n",
    "        self.Ny = Ny\n",
    "        self.FC1Fin = CL2_F*(Nx//4)**2\n",
    "        \n",
    "        self.WCL1 = self.init_weights([CL1_K,CL1_K,1,CL1_F],CL1_K**2,CL1_F)\n",
    "        self.bCL1 = tf.zeros([CL1_F],tf.float32)\n",
    "        self.WCL2 = self.init_weights([CL2_K,CL2_K,CL1_F,CL2_F],CL1_F*CL2_K**2,CL2_F)\n",
    "        self.bCL2 = tf.zeros([CL2_F],tf.float32)\n",
    "        self.WFC1 = self.init_weights([CL2_F*(Nx//4)**2,FC1_F],CL2_F*(Nx//4)**2,FC1_F)\n",
    "        self.bFC1 = tf.zeros([FC1_F],tf.float32)\n",
    "        self.WFC2 = self.init_weights([FC1_F,FC2_F],FC1_F,FC2_F)\n",
    "        self.bFC2 = tf.zeros([FC2_F],tf.float32)\n",
    "        \n",
    "        # Variables for the computational graph\n",
    "        self.WCL1 = tf.Variable(self.WCL1)\n",
    "        self.bCL1 = tf.Variable(self.bCL1)\n",
    "        self.WCL2 = tf.Variable(self.WCL2)\n",
    "        self.bCL2 = tf.Variable(self.bCL2)\n",
    "        self.WFC1 = tf.Variable(self.WFC1)\n",
    "        self.bFC1 = tf.Variable(self.bFC1)\n",
    "        self.WFC2 = tf.Variable(self.WFC2)\n",
    "        self.bFC2 = tf.Variable(self.bFC2)\n",
    "           \n",
    "        \n",
    "    def init_weights(self, shape, Fin, Fout):\n",
    "    \n",
    "        scale = tf.sqrt( 2.0/ (Fin+Fout) )\n",
    "        W = tf.random_uniform( shape, minval=-scale, maxval=scale ) \n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def conv2d(self, x, W):\n",
    "        \n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    \n",
    "    def max_pool_2x2(self, x):\n",
    "        \n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    \n",
    "    def forward(self, x, d):\n",
    "        \n",
    "        # CL1\n",
    "        x = tf.reshape(x, [-1,self.Ny,self.Nx,1])\n",
    "        x = self.conv2d(x, self.WCL1) + self.bCL1\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.max_pool_2x2(x)\n",
    "            \n",
    "        # CL2\n",
    "        x = self.conv2d(x, self.WCL2) + self.bCL2\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.max_pool_2x2(x)\n",
    "\n",
    "        # FC1\n",
    "        x = tf.reshape(x, [-1,self.FC1Fin])\n",
    "        x = tf.matmul(x, self.WFC1) + self.bFC1\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.dropout(x, d)\n",
    "\n",
    "        # FC2\n",
    "        x = tf.matmul(x, self.WFC2) + self.bFC2\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def loss(self, x, x_target, l2_regularization): \n",
    "        \n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=x_target, logits=x)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        l2_loss = 0.0\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            l2_loss += tf.nn.l2_loss(var) \n",
    "\n",
    "        loss += l2_regularization* l2_loss\n",
    "           \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def backward(self, loss, learning_rate, train_size, batch_size):            \n",
    "\n",
    "        batch = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate,batch * batch_size, train_size, 0.95,\n",
    "                staircase=True)\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)  \n",
    "        backward = optimizer.minimize(loss, global_step=batch) \n",
    "        \n",
    "        return backward, learning_rate\n",
    "    \n",
    "    \n",
    "    def evaluation(self, x, x_target):\n",
    "        \n",
    "        predicted_classes = tf.cast( tf.argmax( tf.nn.softmax(x), 1 ), tf.int32 )\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_classes,x_target), tf.float32))\n",
    "        \n",
    "        return 100.0* accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing network to delete\n",
      "\n",
      "ConvNet: LeNet5\n",
      "num_epochs= 20 , train_size= 55000 , nb_iter= 11000\n",
      "WARNING:tensorflow:From <ipython-input-4-530b330f37f8>:47 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "epoch= 1, i=  100, loss(batch)= 0.2747, accuray(batch)= 94.00\n",
      "epoch= 1, i=  200, loss(batch)= 0.1452, accuray(batch)= 98.00\n",
      "epoch= 1, i=  300, loss(batch)= 0.1559, accuray(batch)= 99.00\n",
      "epoch= 1, i=  400, loss(batch)= 0.2485, accuray(batch)= 97.00\n",
      "epoch= 1, i=  500, loss(batch)= 0.1383, accuray(batch)= 99.00\n",
      "epoch= 1, loss(train)= 0.313, accuracy(train)= 93.369, time= 6.836, lr= 0.05000\n",
      "  accuracy(test) = 98.620 %, time= 0.390\n",
      "epoch= 2, i=  100, loss(batch)= 0.1222, accuray(batch)= 100.00\n",
      "epoch= 2, i=  200, loss(batch)= 0.1786, accuray(batch)= 97.00\n",
      "epoch= 2, i=  300, loss(batch)= 0.1771, accuray(batch)= 97.00\n",
      "epoch= 2, i=  400, loss(batch)= 0.1606, accuray(batch)= 98.00\n",
      "epoch= 2, i=  500, loss(batch)= 0.1377, accuray(batch)= 99.00\n",
      "epoch= 2, loss(train)= 0.154, accuracy(train)= 98.276, time= 6.439, lr= 0.04750\n",
      "  accuracy(test) = 99.140 %, time= 0.369\n",
      "epoch= 3, i=  100, loss(batch)= 0.1078, accuray(batch)= 100.00\n",
      "epoch= 3, i=  200, loss(batch)= 0.1211, accuray(batch)= 99.00\n",
      "epoch= 3, i=  300, loss(batch)= 0.1261, accuray(batch)= 99.00\n",
      "epoch= 3, i=  400, loss(batch)= 0.1630, accuray(batch)= 99.00\n",
      "epoch= 3, i=  500, loss(batch)= 0.1237, accuray(batch)= 98.00\n",
      "epoch= 3, loss(train)= 0.130, accuracy(train)= 98.765, time= 6.428, lr= 0.04512\n",
      "  accuracy(test) = 99.070 %, time= 0.377\n",
      "epoch= 4, i=  100, loss(batch)= 0.1206, accuray(batch)= 98.00\n",
      "epoch= 4, i=  200, loss(batch)= 0.1007, accuray(batch)= 99.00\n",
      "epoch= 4, i=  300, loss(batch)= 0.1372, accuray(batch)= 97.00\n",
      "epoch= 4, i=  400, loss(batch)= 0.1192, accuray(batch)= 98.00\n",
      "epoch= 4, i=  500, loss(batch)= 0.1044, accuray(batch)= 98.00\n",
      "epoch= 4, loss(train)= 0.113, accuracy(train)= 98.898, time= 6.424, lr= 0.04287\n",
      "  accuracy(test) = 99.170 %, time= 0.388\n",
      "epoch= 5, i=  100, loss(batch)= 0.0903, accuray(batch)= 99.00\n",
      "epoch= 5, i=  200, loss(batch)= 0.0968, accuray(batch)= 99.00\n",
      "epoch= 5, i=  300, loss(batch)= 0.1010, accuray(batch)= 99.00\n",
      "epoch= 5, i=  400, loss(batch)= 0.0873, accuray(batch)= 99.00\n",
      "epoch= 5, i=  500, loss(batch)= 0.1048, accuray(batch)= 99.00\n",
      "epoch= 5, loss(train)= 0.101, accuracy(train)= 99.084, time= 6.428, lr= 0.04073\n",
      "  accuracy(test) = 99.330 %, time= 0.371\n",
      "epoch= 6, i=  100, loss(batch)= 0.0812, accuray(batch)= 100.00\n",
      "epoch= 6, i=  200, loss(batch)= 0.0959, accuray(batch)= 99.00\n",
      "epoch= 6, i=  300, loss(batch)= 0.0736, accuray(batch)= 100.00\n",
      "epoch= 6, i=  400, loss(batch)= 0.0698, accuray(batch)= 100.00\n",
      "epoch= 6, i=  500, loss(batch)= 0.1455, accuray(batch)= 97.00\n",
      "epoch= 6, loss(train)= 0.089, accuracy(train)= 99.256, time= 6.435, lr= 0.03869\n",
      "  accuracy(test) = 99.250 %, time= 0.371\n",
      "epoch= 7, i=  100, loss(batch)= 0.0904, accuray(batch)= 99.00\n",
      "epoch= 7, i=  200, loss(batch)= 0.0654, accuray(batch)= 100.00\n",
      "epoch= 7, i=  300, loss(batch)= 0.0912, accuray(batch)= 99.00\n",
      "epoch= 7, i=  400, loss(batch)= 0.0637, accuray(batch)= 100.00\n",
      "epoch= 7, i=  500, loss(batch)= 0.0650, accuray(batch)= 100.00\n",
      "epoch= 7, loss(train)= 0.085, accuracy(train)= 99.222, time= 6.438, lr= 0.03675\n",
      "  accuracy(test) = 99.370 %, time= 0.364\n",
      "epoch= 8, i=  100, loss(batch)= 0.0758, accuray(batch)= 100.00\n",
      "epoch= 8, i=  200, loss(batch)= 0.0681, accuray(batch)= 100.00\n",
      "epoch= 8, i=  300, loss(batch)= 0.0898, accuray(batch)= 99.00\n",
      "epoch= 8, i=  400, loss(batch)= 0.0584, accuray(batch)= 100.00\n",
      "epoch= 8, i=  500, loss(batch)= 0.0593, accuray(batch)= 100.00\n",
      "epoch= 8, loss(train)= 0.077, accuracy(train)= 99.367, time= 6.417, lr= 0.03492\n",
      "  accuracy(test) = 99.300 %, time= 0.381\n",
      "epoch= 9, i=  100, loss(batch)= 0.0595, accuray(batch)= 100.00\n",
      "epoch= 9, i=  200, loss(batch)= 0.1057, accuray(batch)= 97.00\n",
      "epoch= 9, i=  300, loss(batch)= 0.0696, accuray(batch)= 99.00\n",
      "epoch= 9, i=  400, loss(batch)= 0.0581, accuray(batch)= 100.00\n",
      "epoch= 9, i=  500, loss(batch)= 0.0736, accuray(batch)= 99.00\n",
      "epoch= 9, loss(train)= 0.071, accuracy(train)= 99.445, time= 6.416, lr= 0.03317\n",
      "  accuracy(test) = 99.280 %, time= 0.381\n",
      "epoch= 10, i=  100, loss(batch)= 0.0833, accuray(batch)= 98.00\n",
      "epoch= 10, i=  200, loss(batch)= 0.0680, accuray(batch)= 99.00\n",
      "epoch= 10, i=  300, loss(batch)= 0.0747, accuray(batch)= 99.00\n",
      "epoch= 10, i=  400, loss(batch)= 0.0503, accuray(batch)= 100.00\n",
      "epoch= 10, i=  500, loss(batch)= 0.0777, accuray(batch)= 99.00\n",
      "epoch= 10, loss(train)= 0.068, accuracy(train)= 99.462, time= 6.425, lr= 0.03151\n",
      "  accuracy(test) = 99.290 %, time= 0.387\n",
      "epoch= 11, i=  100, loss(batch)= 0.0612, accuray(batch)= 99.00\n",
      "epoch= 11, i=  200, loss(batch)= 0.0513, accuray(batch)= 100.00\n",
      "epoch= 11, i=  300, loss(batch)= 0.0865, accuray(batch)= 99.00\n",
      "epoch= 11, i=  400, loss(batch)= 0.0481, accuray(batch)= 100.00\n",
      "epoch= 11, i=  500, loss(batch)= 0.0831, accuray(batch)= 98.00\n",
      "epoch= 11, loss(train)= 0.064, accuracy(train)= 99.464, time= 6.423, lr= 0.02994\n",
      "  accuracy(test) = 99.310 %, time= 0.384\n",
      "epoch= 12, i=  100, loss(batch)= 0.0704, accuray(batch)= 99.00\n",
      "epoch= 12, i=  200, loss(batch)= 0.0542, accuray(batch)= 100.00\n",
      "epoch= 12, i=  300, loss(batch)= 0.0518, accuray(batch)= 100.00\n",
      "epoch= 12, i=  400, loss(batch)= 0.0484, accuray(batch)= 100.00\n",
      "epoch= 12, i=  500, loss(batch)= 0.0535, accuray(batch)= 100.00\n",
      "epoch= 12, loss(train)= 0.061, accuracy(train)= 99.520, time= 6.424, lr= 0.02844\n",
      "  accuracy(test) = 99.310 %, time= 0.369\n",
      "epoch= 13, i=  100, loss(batch)= 0.0513, accuray(batch)= 100.00\n",
      "epoch= 13, i=  200, loss(batch)= 0.0507, accuray(batch)= 100.00\n",
      "epoch= 13, i=  300, loss(batch)= 0.0568, accuray(batch)= 99.00\n",
      "epoch= 13, i=  400, loss(batch)= 0.0454, accuray(batch)= 100.00\n",
      "epoch= 13, i=  500, loss(batch)= 0.0555, accuray(batch)= 99.00\n",
      "epoch= 13, loss(train)= 0.058, accuracy(train)= 99.551, time= 6.424, lr= 0.02702\n",
      "  accuracy(test) = 99.280 %, time= 0.360\n",
      "epoch= 14, i=  100, loss(batch)= 0.0621, accuray(batch)= 99.00\n",
      "epoch= 14, i=  200, loss(batch)= 0.0484, accuray(batch)= 100.00\n",
      "epoch= 14, i=  300, loss(batch)= 0.0492, accuray(batch)= 100.00\n",
      "epoch= 14, i=  400, loss(batch)= 0.0423, accuray(batch)= 100.00\n",
      "epoch= 14, i=  500, loss(batch)= 0.0803, accuray(batch)= 99.00\n",
      "epoch= 14, loss(train)= 0.055, accuracy(train)= 99.616, time= 6.414, lr= 0.02567\n",
      "  accuracy(test) = 99.310 %, time= 0.373\n",
      "epoch= 15, i=  100, loss(batch)= 0.0604, accuray(batch)= 99.00\n",
      "epoch= 15, i=  200, loss(batch)= 0.0695, accuray(batch)= 99.00\n",
      "epoch= 15, i=  300, loss(batch)= 0.0498, accuray(batch)= 100.00\n",
      "epoch= 15, i=  400, loss(batch)= 0.0511, accuray(batch)= 99.00\n",
      "epoch= 15, i=  500, loss(batch)= 0.0489, accuray(batch)= 100.00\n",
      "epoch= 15, loss(train)= 0.054, accuracy(train)= 99.625, time= 6.423, lr= 0.02438\n",
      "  accuracy(test) = 99.420 %, time= 0.385\n",
      "epoch= 16, i=  100, loss(batch)= 0.0435, accuray(batch)= 100.00\n",
      "epoch= 16, i=  200, loss(batch)= 0.0419, accuray(batch)= 100.00\n",
      "epoch= 16, i=  300, loss(batch)= 0.0799, accuray(batch)= 98.00\n",
      "epoch= 16, i=  400, loss(batch)= 0.0516, accuray(batch)= 99.00\n",
      "epoch= 16, i=  500, loss(batch)= 0.0565, accuray(batch)= 98.00\n",
      "epoch= 16, loss(train)= 0.051, accuracy(train)= 99.636, time= 6.422, lr= 0.02316\n",
      "  accuracy(test) = 99.300 %, time= 0.385\n",
      "epoch= 17, i=  100, loss(batch)= 0.0633, accuray(batch)= 99.00\n",
      "epoch= 17, i=  200, loss(batch)= 0.0395, accuray(batch)= 100.00\n",
      "epoch= 17, i=  300, loss(batch)= 0.0436, accuray(batch)= 100.00\n",
      "epoch= 17, i=  400, loss(batch)= 0.0516, accuray(batch)= 100.00\n",
      "epoch= 17, i=  500, loss(batch)= 0.0410, accuray(batch)= 100.00\n",
      "epoch= 17, loss(train)= 0.050, accuracy(train)= 99.642, time= 6.420, lr= 0.02201\n",
      "  accuracy(test) = 99.260 %, time= 0.362\n",
      "epoch= 18, i=  100, loss(batch)= 0.0484, accuray(batch)= 100.00\n",
      "epoch= 18, i=  200, loss(batch)= 0.0395, accuray(batch)= 100.00\n",
      "epoch= 18, i=  300, loss(batch)= 0.0412, accuray(batch)= 100.00\n",
      "epoch= 18, i=  400, loss(batch)= 0.0601, accuray(batch)= 99.00\n",
      "epoch= 18, i=  500, loss(batch)= 0.0793, accuray(batch)= 99.00\n",
      "epoch= 18, loss(train)= 0.048, accuracy(train)= 99.715, time= 6.415, lr= 0.02091\n",
      "  accuracy(test) = 99.370 %, time= 0.378\n",
      "epoch= 19, i=  100, loss(batch)= 0.0455, accuray(batch)= 100.00\n",
      "epoch= 19, i=  200, loss(batch)= 0.0380, accuray(batch)= 100.00\n",
      "epoch= 19, i=  300, loss(batch)= 0.0507, accuray(batch)= 99.00\n",
      "epoch= 19, i=  400, loss(batch)= 0.0742, accuray(batch)= 99.00\n",
      "epoch= 19, i=  500, loss(batch)= 0.0408, accuray(batch)= 100.00\n",
      "epoch= 19, loss(train)= 0.047, accuracy(train)= 99.715, time= 6.429, lr= 0.01986\n",
      "  accuracy(test) = 99.330 %, time= 0.379\n",
      "epoch= 20, i=  100, loss(batch)= 0.0374, accuray(batch)= 100.00\n",
      "epoch= 20, i=  200, loss(batch)= 0.0416, accuray(batch)= 100.00\n",
      "epoch= 20, i=  300, loss(batch)= 0.0364, accuray(batch)= 100.00\n",
      "epoch= 20, i=  400, loss(batch)= 0.0427, accuray(batch)= 100.00\n",
      "epoch= 20, i=  500, loss(batch)= 0.0645, accuray(batch)= 98.00\n",
      "epoch= 20, loss(train)= 0.046, accuracy(train)= 99.695, time= 6.436, lr= 0.01887\n",
      "  accuracy(test) = 99.340 %, time= 0.369\n"
     ]
    }
   ],
   "source": [
    "# Delete existing network if exists\n",
    "try:\n",
    "    del net\n",
    "    tf.reset_default_graph() \n",
    "    print('Delete existing network\\n')\n",
    "except NameError:\n",
    "    print('No existing network to delete\\n')\n",
    "\n",
    "    \n",
    "# network parameters\n",
    "Nx = Ny = 28\n",
    "CL1_F = 32\n",
    "CL1_K = 5\n",
    "CL2_F = 64\n",
    "CL2_K = 5\n",
    "FC1_F = 512\n",
    "FC2_F = 10\n",
    "net_parameters = [Nx, Ny, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]\n",
    "\n",
    "# instantiate the object net of the class \n",
    "net = ConvNet_LeNet5(net_parameters)\n",
    "\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.05\n",
    "dropout_value = 0.5\n",
    "l2_regularization = 5e-4 \n",
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "train_size = train_data.shape[0]\n",
    "nb_iter = int(num_epochs * train_size) // batch_size\n",
    "print('num_epochs=',num_epochs,', train_size=',train_size,', nb_iter=',nb_iter)\n",
    "\n",
    "\n",
    "# computational graph\n",
    "x = tf.placeholder(tf.float32, (None, 784))\n",
    "x_target = tf.placeholder(tf.int32, (None))\n",
    "d = tf.placeholder(tf.float32)\n",
    "\n",
    "x_score = net.forward(x,d)\n",
    "loss = net.loss(x_score,x_target,l2_regularization)\n",
    "backward, lr_op = net.backward(loss,learning_rate,train_size,batch_size)\n",
    "evaluation = net.evaluation(x_score,x_target)\n",
    "\n",
    "\n",
    "# train\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "# loop over epochs\n",
    "indices = collections.deque()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # reshuffle \n",
    "    indices.extend(np.random.permutation(train_size)) # rand permutation\n",
    "    \n",
    "    # reset time\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # extract batches\n",
    "    running_loss = 0.0\n",
    "    running_accuray = 0\n",
    "    running_total = 0\n",
    "    while len(indices) >= batch_size:\n",
    "        \n",
    "        # extract batches\n",
    "        batch_idx = [indices.popleft() for i in range(batch_size)]\n",
    "        batch_x, batch_y = train_data[batch_idx,:], train_labels[batch_idx]\n",
    "        batch_y = np.array(batch_y,np.int32)\n",
    "        if type(batch_x) is not np.ndarray:\n",
    "            batch_x = batch_x.toarray()  # convert to full matrices if sparse\n",
    "        \n",
    "        # run computational graph \n",
    "        _,acc_train,loss_train,lr_train = sess.run([backward,evaluation,loss,lr_op], feed_dict={x: batch_x, x_target: batch_y, d: dropout_value})\n",
    "        \n",
    "        # loss, accuracy\n",
    "        running_loss += loss_train\n",
    "        running_accuray += acc_train\n",
    "        running_total += 1\n",
    "        \n",
    "        # print        \n",
    "        if not running_total%100: # print every x mini-batches\n",
    "            print('epoch= %d, i= %4d, loss(batch)= %.4f, accuray(batch)= %.2f' % (epoch+1, running_total, loss_train, acc_train))\n",
    "        \n",
    "\n",
    "    t_stop = time.time() - t_start\n",
    "    print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' % \n",
    "          (epoch+1, running_loss/running_total, running_accuray/running_total, t_stop, lr_train))\n",
    " \n",
    "\n",
    "    # Test set\n",
    "    running_accuray_test = 0\n",
    "    running_total_test = 0\n",
    "    indices_test = collections.deque()\n",
    "    indices_test.extend(range(test_data.shape[0]))\n",
    "    t_start_test = time.time()\n",
    "    while len(indices_test) >= batch_size:\n",
    "        batch_idx_test = [indices_test.popleft() for i in range(batch_size)]\n",
    "        batch_x_test, batch_y_test = test_data[batch_idx_test,:], test_labels[batch_idx_test]\n",
    "        batch_y_test = np.array(batch_y_test,np.int32)\n",
    "        if type(batch_x_test) is not np.ndarray:\n",
    "            batch_x_test = batch_x_test.toarray()  # convert to full matrices if sparse\n",
    "        acc_test = sess.run(evaluation, feed_dict={x: batch_x_test, x_target: batch_y_test, d: 1.0})\n",
    "        running_accuray_test += acc_test\n",
    "        running_total_test += 1\n",
    "    t_stop_test = time.time() - t_start_test\n",
    "    print('  accuracy(test) = %.3f %%, time= %.3f' % (running_accuray_test / running_total_test, t_stop_test))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
