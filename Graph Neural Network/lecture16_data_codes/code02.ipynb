{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Course\n",
    "### &nbsp; &nbsp; &nbsp; Xavier Bresson, Sept. 2017\n",
    "## Lecture on Deep Learning on Graphs\n",
    "## Code 2 : Graph LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of CNNs on graphs<br>\n",
    "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering<br>\n",
    "M Defferrard, X Bresson, P Vandergheynst<br>\n",
    "arXiv preprint arXiv:1606.09375<br>\n",
    "NIPS 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'lib/')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000,)\n",
      "(5000, 784)\n",
      "(5000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('datasets', one_hot=False) # load data in folder datasets/\n",
    "\n",
    "train_data = mnist.train.images.astype(np.float32)\n",
    "val_data = mnist.validation.images.astype(np.float32)\n",
    "test_data = mnist.test.images.astype(np.float32)\n",
    "train_labels = mnist.train.labels\n",
    "val_labels = mnist.validation.labels\n",
    "test_labels = mnist.test.labels\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb edges:  6396\n",
      "Graclus coarsening with Xavier version\n",
      "Layer 0: M_0 = |V| = 976 nodes (192 added), |E| = 3198 edges\n",
      "Layer 1: M_1 = |V| = 488 nodes (84 added), |E| = 1593 edges\n",
      "Layer 2: M_2 = |V| = 244 nodes (31 added), |E| = 766 edges\n",
      "Layer 3: M_3 = |V| = 122 nodes (9 added), |E| = 381 edges\n",
      "Layer 4: M_4 = |V| = 61 nodes (0 added), |E| = 195 edges\n",
      "lmax: [1.3857559, 1.3440365, 1.1847966, 1.0000006]\n",
      "(55000, 976)\n",
      "(5000, 976)\n",
      "(10000, 976)\n",
      "Execution time: 2.00s\n"
     ]
    }
   ],
   "source": [
    "from lib.grid_graph import grid_graph\n",
    "from lib.coarsening import coarsen\n",
    "from lib.coarsening import lmaxX\n",
    "from lib.coarsening import perm_data\n",
    "from lib.coarsening import lmaxX\n",
    "from lib.coarsening import rescale_L\n",
    "\n",
    "# Construct graph\n",
    "t_start = time.time()\n",
    "grid_side = 28\n",
    "number_edges = 8\n",
    "metric = 'euclidean'\n",
    "A = grid_graph(grid_side,number_edges,metric) # create graph of Euclidean grid\n",
    "\n",
    "# Compute coarsened graphs\n",
    "coarsening_levels = 4\n",
    "L, perm = coarsen(A, coarsening_levels)\n",
    "\n",
    "# Compute max eigenvalue of graph Laplacians\n",
    "lmax = []\n",
    "for i in range(coarsening_levels):\n",
    "    lmax.append(lmaxX(L[i]))\n",
    "print('lmax: ' + str([lmax[i] for i in range(coarsening_levels)]))\n",
    "\n",
    "# Reindex nodes to satisfy a binary tree structure\n",
    "train_data = perm_data(train_data, perm)\n",
    "val_data = perm_data(val_data, perm)\n",
    "test_data = perm_data(test_data, perm)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "print('Execution time: {:.2f}s'.format(time.time() - t_start))\n",
    "del perm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph ConvNet LeNet5\n",
    "### CL32-MP4-CL64-MP4-FC512-FC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Graph_ConvNet_LeNet5(object):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, net_parameters):\n",
    "        \n",
    "        print('Graph ConvNet: LeNet5')\n",
    "        \n",
    "        # parameters\n",
    "        D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F = net_parameters\n",
    "        \n",
    "        # init\n",
    "        self.WCL1 = self.init_weights([CL1_K, CL1_F],CL1_K,CL1_F)\n",
    "        self.bCL1 = tf.zeros([CL1_F],tf.float32)\n",
    "        self.CL1_F = CL1_F; self.CL1_K = CL1_K\n",
    "        self.WCL2 = self.init_weights([CL2_K*CL1_F,CL2_F],CL2_K*CL1_F,CL2_F)\n",
    "        self.bCL2 = tf.zeros([CL2_F],tf.float32)\n",
    "        self.CL2_F = CL2_F; self.CL2_K = CL2_K\n",
    "        self.WFC1 = self.init_weights([CL2_F*(D//16),FC1_F],CL2_F*(D//16),FC1_F)\n",
    "        self.bFC1 = tf.zeros([FC1_F],tf.float32)\n",
    "        self.WFC2 = self.init_weights([FC1_F,FC2_F],FC1_F,FC2_F)\n",
    "        self.bFC2 = tf.zeros([FC2_F],tf.float32)\n",
    "        \n",
    "        # Variables for the computational graph\n",
    "        self.WCL1 = tf.Variable(self.WCL1)\n",
    "        self.bCL1 = tf.Variable(self.bCL1)\n",
    "        self.WCL2 = tf.Variable(self.WCL2)\n",
    "        self.bCL2 = tf.Variable(self.bCL2)\n",
    "        self.WFC1 = tf.Variable(self.WFC1)\n",
    "        self.bFC1 = tf.Variable(self.bFC1)\n",
    "        self.WFC2 = tf.Variable(self.WFC2)\n",
    "        self.bFC2 = tf.Variable(self.bFC2)\n",
    "        \n",
    "        \n",
    "    def init_weights(self, shape, Fin, Fout):\n",
    "    \n",
    "        scale = tf.sqrt( 2.0/ (Fin+Fout) )\n",
    "        W = tf.random_uniform( shape, minval=-scale, maxval=scale ) \n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def graph_conv_cheby(self, x, W, L, lmax, Fout, K):\n",
    "        \n",
    "        # parameters\n",
    "        # B = batch size\n",
    "        # V = nb vertices\n",
    "        # Fin = nb input features\n",
    "        # Fout = nb output features\n",
    "        # K = Chebyshev order & support size\n",
    "        B, V, Fin = x.get_shape(); B, V, Fin = int(B), int(V), int(Fin) \n",
    "\n",
    "        # rescale Laplacian\n",
    "        lmax = lmaxX(L)\n",
    "        L = rescale_L(L, lmax) \n",
    "        \n",
    "        # scipy sparse matric of L\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col)) \n",
    "        L = tf.SparseTensor(indices, L.data, L.shape) \n",
    "        L = tf.sparse_reorder(L)\n",
    "        \n",
    "        # Transform to Chebyshev basis\n",
    "        x0 = tf.transpose(x, perm=[1, 2, 0])  # V x Fin x B\n",
    "        x0 = tf.reshape(x0, [V, Fin*B])       # V x Fin*B\n",
    "        x = tf.expand_dims(x0, 0)             # 1 x V x Fin*B\n",
    "        def concat(x, x_):\n",
    "            x_ = tf.expand_dims(x_, 0)        # 1 x V x Fin*B\n",
    "            return tf.concat(0, [x, x_])      # K x V x Fin*B      \n",
    "        if K > 1: \n",
    "            x1 = tf.sparse_tensor_dense_matmul(L, x0) \n",
    "            x = concat(x, x1) \n",
    "        for k in range(2, K):\n",
    "            x2 = 2 * tf.sparse_tensor_dense_matmul(L, x1) - x0  \n",
    "            x = concat(x, x2)                 # M x Fin*B\n",
    "            x0, x1 = x1, x2  \n",
    "        x = tf.reshape(x, [K, V, Fin, B])     # K x V x Fin x B\n",
    "        x = tf.transpose(x, perm=[3,1,2,0])   # B x V x Fin x K\n",
    "        x = tf.reshape(x, [B*V, Fin*K])       # B*V x Fin*K\n",
    "        \n",
    "        # Compose linearly Fin features to get Fout features\n",
    "        x = tf.matmul(x, W)                   # B*V x Fout   \n",
    "        x = tf.reshape(x, [B, V, Fout])       # B x V x Fout\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    # Max pooling of size p. Must be a power of 2.\n",
    "    def graph_max_pool_(self, x, p):\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # B x V x F x 1\n",
    "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            return tf.squeeze(x, [3])  # B x V/p x F\n",
    "        else:\n",
    "            return x    \n",
    "        \n",
    "    \n",
    "    def forward(self, x, d, L, lmax):\n",
    "        \n",
    "        # Graph CL1\n",
    "        x = tf.expand_dims(x, 2)  # B x V x Fin=1  \n",
    "        x = self.graph_conv_cheby(x, self.WCL1, L[0], lmax[0], self.CL1_F, self.CL1_K) + self.bCL1\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.graph_max_pool_(x, 4)\n",
    "            \n",
    "        # Graph CL2\n",
    "        x = self.graph_conv_cheby(x, self.WCL2, L[2], lmax[2], self.CL2_F, self.CL2_K) + self.bCL2\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.graph_max_pool_(x, 4)\n",
    "        \n",
    "        # FC1\n",
    "        B, V, F = x.get_shape(); B, V, F = int(B), int(V), int(F)\n",
    "        x = tf.reshape(x, [B, V*F]) \n",
    "        x = tf.matmul(x, self.WFC1) + self.bFC1\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.dropout(x, d)\n",
    "            \n",
    "        # FC2\n",
    "        x = tf.matmul(x, self.WFC2) + self.bFC2\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def loss(self, x, x_target, l2_regularization): \n",
    "        \n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=x_target, logits=x)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        l2_loss = 0.0\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            l2_loss += tf.nn.l2_loss(var) \n",
    "        \n",
    "        loss += l2_regularization* l2_loss\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def backward(self, loss, learning_rate, train_size, batch_size):            \n",
    "\n",
    "        batch = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate,batch * batch_size, train_size, 0.95,\n",
    "                staircase=True)\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)  \n",
    "        backward = optimizer.minimize(loss, global_step=batch) \n",
    "        \n",
    "        return backward, learning_rate\n",
    "    \n",
    "    \n",
    "    def evaluation(self, x, x_target):\n",
    "        \n",
    "        predicted_classes = tf.cast( tf.argmax( tf.nn.softmax(x), 1 ), tf.int32 )\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_classes,x_target), tf.float32))\n",
    "        \n",
    "        return 100.0* accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Delete existing network if exists\n",
    "try:\n",
    "    del net\n",
    "    tf.reset_default_graph() \n",
    "    print('Delete existing network\\n')\n",
    "except NameError:\n",
    "    print('No existing network to delete\\n')\n",
    "\n",
    "    \n",
    "# network parameters\n",
    "D = train_data.shape[1]\n",
    "CL1_F = 32\n",
    "CL1_K = 25\n",
    "CL2_F = 64\n",
    "CL2_K = 25\n",
    "FC1_F = 512\n",
    "FC2_F = 10\n",
    "net_parameters = [D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]\n",
    "\n",
    "# instantiate the object net of the class \n",
    "net = Graph_ConvNet_LeNet5(net_parameters)\n",
    "\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.05\n",
    "dropout_value = 0.5\n",
    "l2_regularization = 5e-4 \n",
    "batch_size = 100\n",
    "num_epochs = 30\n",
    "train_size = train_data.shape[0]\n",
    "nb_iter = int(num_epochs * train_size) // batch_size\n",
    "print('num_epochs=',num_epochs,', train_size=',train_size,', nb_iter=',nb_iter)\n",
    "\n",
    "\n",
    "# computational graph\n",
    "x = tf.placeholder(tf.float32, (batch_size, D))\n",
    "x_target = tf.placeholder(tf.int32, (batch_size))\n",
    "d = tf.placeholder(tf.float32)\n",
    "\n",
    "x_score = net.forward(x,d,L,lmax)\n",
    "loss = net.loss(x_score,x_target,l2_regularization)\n",
    "backward, lr_op = net.backward(loss,learning_rate,train_size,batch_size)\n",
    "evaluation = net.evaluation(x_score,x_target)\n",
    "\n",
    "\n",
    "# train\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "# loop over epochs\n",
    "indices = collections.deque()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # reshuffle \n",
    "    indices.extend(np.random.permutation(train_size)) # rand permutation\n",
    "    \n",
    "    # reset time\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # extract batches\n",
    "    running_loss = 0.0\n",
    "    running_accuray = 0\n",
    "    running_total = 0\n",
    "    while len(indices) >= batch_size:\n",
    "        \n",
    "        # extract batches\n",
    "        batch_idx = [indices.popleft() for i in range(batch_size)]\n",
    "        batch_x, batch_y = train_data[batch_idx,:], train_labels[batch_idx]\n",
    "        batch_y = np.array(batch_y,np.int32)\n",
    "        if type(batch_x) is not np.ndarray:\n",
    "            batch_x = batch_x.toarray()  # convert to full matrices if sparse\n",
    "\n",
    "        # run computational graph \n",
    "        _,acc_train,loss_train,lr_train = sess.run([backward,evaluation,loss,lr_op], feed_dict={x: batch_x, x_target: batch_y, d: dropout_value})\n",
    "        \n",
    "        # loss, accuracy\n",
    "        running_loss += loss_train\n",
    "        running_accuray += acc_train\n",
    "        running_total += 1\n",
    "        \n",
    "        # print        \n",
    "        if not running_total%100: # print every x mini-batches\n",
    "            print('epoch= %d, i= %4d, loss(batch)= %.3f, accuray(batch)= %.2f' % (epoch+1, running_total, loss_train, acc_train))\n",
    "        \n",
    "    t_stop = time.time() - t_start\n",
    "    print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' % \n",
    "          (epoch+1, running_loss/running_total, running_accuray/running_total, t_stop, lr_train))\n",
    "\n",
    "    \n",
    "    # Test set\n",
    "    running_accuray_test = 0\n",
    "    running_total_test = 0\n",
    "    indices_test = collections.deque()\n",
    "    indices_test.extend(range(test_data.shape[0]))\n",
    "    t_start_test = time.time()\n",
    "    while len(indices_test) >= batch_size:\n",
    "        batch_idx_test = [indices_test.popleft() for i in range(batch_size)]\n",
    "        batch_x_test, batch_y_test = test_data[batch_idx_test,:], test_labels[batch_idx_test]\n",
    "        batch_y_test = np.array(batch_y_test,np.int32)\n",
    "        if type(batch_x_test) is not np.ndarray:\n",
    "            batch_x_test = batch_x_test.toarray()  # convert to full matrices if sparse\n",
    "        acc_test = sess.run(evaluation, feed_dict={x: batch_x_test, x_target: batch_y_test, d: 1.0})\n",
    "        running_accuray_test += acc_test\n",
    "        running_total_test += 1\n",
    "    t_stop_test = time.time() - t_start_test\n",
    "    print('  accuracy(test) = %.3f %%, time= %.3f' % (running_accuray_test / running_total_test, t_stop_test))  \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Click here to get the html version of the notebook](html_notebooks/code02.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
